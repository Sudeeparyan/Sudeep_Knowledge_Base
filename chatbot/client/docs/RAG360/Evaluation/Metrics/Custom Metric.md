## Custom Metric Creation Workflow in Trulens

### Prerequisite Knowledge

1. Feedback Functions in Trulens
2. Providers in Trulens
3. Records in Trulens
4. Selectors in Trulens

### Procedure

1. Create a python class that inherits the desired provider(OpenAI or
   AzureOpenAI) class.
2. In the class, define a function that takes inputs and provides any float
   value between 0 to 1 as the output.
3. The function can be purely logical or can use AI to come up with the value of
   the metric between 0 to 1.
4. Example of a purely logical function/metric - Identifying lint errors in a
   code snippet using python libraries and producing a score based on the number
   of errors and the number of lines in the code.

```js
def _lint_check_cpp(self, file_path: str):
    import subprocess

    result = subprocess.run(
        ["poetry", "run", "cpplint", file_path],
        stdout=subprocess.PIPE,
        stderr=subprocess.PIPE,
    )
    number_of_errors = result.stdout.decode("utf-8").split("\n")
    error_number = 0
    for line in number_of_errors:
        if "Total errors found" in line:
            errors = int(line.split(":")[1].strip())
            if errors == 0:
                error_number = 0
            else:
                error_number = errors
    error_list = result.stderr.decode("utf-8")
    return error_number, error_list

def lint_check(self, candidate: str, language: str):
    if language == "cpp":
        with open(TEST_CPP_FILE_PATH, "w") as f:
            f.write(candidate)
        result = self._lint_check_cpp(TEST_CPP_FILE_PATH)
        if result[0] == 0:
            return 1.0, {"reason": "No linting errors found"}
        else:
            number_of_lines = len(candidate.split("\n"))
            linting_score = 1 - (result[0] / number_of_lines)
            if linting_score < 0:
                linting_score = 0.0
            return linting_score, {
                "reason": (
                    f"{'Number of Errors: ' + str(result[0])}\n"
                    f"{'List of Errors: ' + str(result[1])}"
                )
            }
```

5. Example of using AI to come up with scores: Calculating how accurate a
   generated search query is with respect to the actual query.

````js
SEARCH_QUERY_ACCURACY_TEMPLATE = """

System:
You are a SEARCH QUERY ACCURACY classifier providing the accuracy of the SEARCH QUERY formed by combination of USER QUESTION and CHAT HISTORY.
You will be provided with a USER QUESTION, CHAT HISTORY and a SEARCH QUERY.
Your task is to evaluate If the SEARCH QUERY accurately reflects the USER QUESTION and includes all necessary details to be understood on its own. This includes appropriately filling in any gaps left by an incomplete USER QUESTION using the CHAT HISTORY.
Purpose of the SEARCH QUERY is to retrieve the desired information from the database related to the USER QUESTION.

USER QUESTION (mentioned within the triple quotes below):
```{user_question}```

CHAT HISTORY (mentioned within the triple quotes below):
```{chat_history}```

SEARCH QUERY (mentioned within the triple quotes below):
```{search_query}```

Please answer using the entire template below.

TEMPLATE:
Observation: <The observation on the criteria for the SEARCH QUERY ACCURACY evaluation.>
Score: <The score 0-10 based on the given criteria>
Supporting Evidence: <Provide your reasons for scoring based on the ACCURACY of the SEARCH QUERY. Tie it back to the evaluation being completed.>
"""
def search_query_accuracy_with_cot_reasons(
    self, user_question: str, chat_history: list[str], search_query: str
) -> float:
    """Get the accuracy score for the search query generated

    Usage:
        search_query_accuracy_feedback = (
            Feedback(provider.search_query_accuracy_with_cot_reasons, name="Search Query Accuracy")
            .on(user_question=Select.RecordCalls.query.args["question"])
            .on(chat_history=Select.RecordCalls.query.args["chat_history"])
            .on(search_query=Select.RecordCalls.get_search_query.rets)
        )


        Args:
            user_question (str): The user question for which the query is being generated
            chat_history (list[str]): previous chat history
            search_query (str): The search query generated by the system

        Returns:
            float: A value between 0 and 1. 0 being "not accurate" and 1 being "mostly accurate".
    """

    system_prompt = str.format(
        SEARCH_QUERY_ACCURACY_TEMPLATE,
        user_question=user_question,
        chat_history=chat_history,
        search_query=search_query,
    )
    return self.generate_score_and_reasons(system_prompt)
````

:::tip

1.  The format of output given in the prompt is important because Trulens will
    help in visualising the output given for 'Supporting Evidence' by the LLM.

2.  The generate_score_and_reasons() function will help in parsing the generated
    text output of LLM, store the metrics and reasons that will help in
    visualizing them in the dashboard later. 
:::

3.  Once we have a function that will provide us a score, we can implement it in
    evaluation by wrapping it as a Feedback Function.
4.  Feedback Functions are individual metrics in Trulens terms.
5.  Feedback Function definition for the above examples:

```js
search_query_accuracy_feedback = Feedback(
  provider.search_query_accuracy_with_cot_reasons,
  (name = "Search Query Accuracy"),
)
  .on((user_question = Select.RecordCalls.query.args["question"]))
  .on((chat_history = Select.RecordCalls.query.args["chat_history"]))
  .on((search_query = Select.RecordCalls.get_search_query.rets));

lint_check = Feedback(provider.lint_check, (name = "Lint Check"))
  .on_output()
  .on(Select.RecordCalls.get_candidate.args["language"]);
```

:::tip 
Note that the arguments to the python metric/function we defined will be
mapped using the on() method and Select object in Trulens which will make use of
attributes of the Record object. 
::: 

9. Now the custom metric can be calculated just as any inbuilt Trulens metric.

### Reference

:::info
[Reference](https://www.trulens.org/trulens_eval/evaluation/feedback_implementations/custom_feedback_functions/)
:::

## Custom Metrics in Astro Chat Assistant

### Custom Metrics in Chat Assistant

<table class="table-size-for-cloud-services">
    <thead>
        <tr>
            <th>Factors</th>
            <th>Description</th>
        </tr>
    </thead>
    <tbody>
        <tr>
            <td class="custom-header">Valid Number of Function Arguments while using Skills</td>
            <td>Validate if the proper number of arguments were provided for the selected skill.</td>
        </tr>
        <tr>
            <td class="custom-header">Correctness of Skill/Function Call made by the Orchestrator</td>
            <td>Check if the choice of skill made by the Orchestrator is the desired skill to answer the given query.</td>
        </tr>
        <tr>
            <td class="custom-header">Guideline Adherence of the Response</td>
            <td>Check if the response generated by LLM follows the guidelines provided in the System Prompt during synthesis.</td>
        </tr>
        <tr>
            <td class="custom-header">Average Moderation Score from OpenAI for the response</td>
            <td>Average of scores provided by Moderation [Hate, Violence, and Harassment/Threatening] API of OpenAI.</td>
        </tr>
    </tbody>
</table>

### Extended Custom Metric Class from LLM Provider

```js
class FunctionsEvaluation(Provider):
    provider: Provider

    def __init__(self, provider: Provider, **kwargs):
        """Initialize the FunctionsEvaluation"""
        super().__init__(provider=provider, **kwargs)

    def evaluate_function_arguments(self, function_arguments, skill_arguments):
        """Evaluate the function arguments"""
        correct_function_arguments = 0
        for function in function_arguments:
            function_name = function["function"]["name"]
            function_args = list(json.loads(function["function"]["arguments"]).keys())
            if function_args == skill_arguments[function_name]:
                correct_function_arguments += 1
        if correct_function_arguments == len(function_arguments) and correct_function_arguments > 0:
            return 1.0
        return 0.0

    def evaluate_function_choice(self, query, function_arguments, skill_arguments):
        """Evaluate the function choice"""
        correctness_scores = []
        if len(function_arguments) == 0:
            return ['{"Correctness": 0}']
        for function in function_arguments:
            function_name = function["function"]["name"]
            correctness_scores.append(
                self.provider._create_chat_completion(
                    prompt=f"{FUNCTION_CHOICE_EVAL_SYSTEM_PROMPT}{FUNCTION_CHOICE_EVALUATION_PROMPT.format(query=query, function_description=skill_arguments[function_name])}"  # noqa
                )
            )
        return correctness_scores

    def evaluate_function_choice_with_cot_reasons(self, query, function_arguments, skill_arguments):
        """Evaluate the function choice with cot reasons"""
        correctness_score = []
        reason = self.evaluate_function_choice(query, function_arguments, skill_arguments)
        for function in reason:
            function = json.loads(function)
            correctness_score.append(re_0_10_rating(str(function["Correctness"])) / 10)
        return np.mean(correctness_score), {"reason": reason}

    def evaluate_response_guideline_adherence(self, query, response, function_responses):
        """Evaluate the response guideline adherence"""
        adherence_scores = []
        reasons = []
        mean = 0.0
        if len(function_responses) > 0:
            context = "\n".join(function_responses)
        else:
            context = "No Context"
        for guideline, criterion in zip(guidelines, criteria):

            reason = self.provider._create_chat_completion(
                prompt=f"{GUIDELINES_ADHERENCE_EVAL_SYSTEM_PROMPT}{GUIDELINE_ADHERENCE_EVALUATION_PROMPT.format(query=query, response=response, context=context, criteria=criterion,guideline=guideline,)}",  # noqa
            )
            reason = json.loads(reason)
            reasons.append(
                {
                    "Guideline": guideline,
                    "CriterionCompliance": reason["CriterionCompliance"],
                    "Feedback": reason["Feedback"],
                    "Adherence": reason["Adherence"],
                }
            )
            if reason["Adherence"] == "YES":
                adherence_scores.append(1)
            elif reason["Adherence"] == "NO":
                adherence_scores.append(0)

        reasons_string = "\n".join([json.dumps(reason) for reason in reasons])
        if len(adherence_scores) > 0:
            mean = round(np.mean(adherence_scores), 2)
        return mean, reasons_string

    def evaluate_guidelines_adherence_with_cot_reasons(self, query, response, function_responses):
        """Evaluate the function choice with cot reasons"""
        score, reason = self.evaluate_response_guideline_adherence(
            query, response, function_responses
        )
        return float(score), {"reason": reason}

    def evaluate_moderation(self, response):
        """Evaluate the moderation hate"""
        score = {}
        score["ModerationHate"] = round(1 - self.provider.moderation_hate(response), 2)
        score["ModerationViolence"] = round(1 - self.provider.moderation_violence(response), 2)
        score["ModerationHarassmentThreatening"] = round(
            1 - self.provider.moderation_harassment_threatening(response), 2
        )
        mean_score = round(np.mean(list(score.values())), 2)
        return mean_score, {"reason": score}

```

### Trulens Recorder Implementation

```js
class OrchestratorTest:
    """Class for TruCustomApp"""

    async def get_answer(self, orchestrator, task):
        return await orchestrator.run(task)

    @instrument
    def query(self, question: str, chat_history: List[str]) -> str:
        """Get the answer to the question"""
        chatHistory: List[str] = chat_history
        ignoredDataSources: List[str] = []
        metaData: TaskMetaData = TaskMetaData()
        attachments: List[Attachment] = []
        additional_input: TaskAdditionalInput = TaskAdditionalInput(
            chatHistory=chatHistory,
            ignoredDataSources=ignoredDataSources,
            metaData=metaData,
            attachments=attachments,
        )

        task = TaskRequestBody(
            input=question,
            additional_input=additional_input,  # noqa
        )

        # Create an instance of OpenAIFunctionCalling
        orchestrator = get_orchestrator_from_config(debug_mode=True)
        result = asyncio.run(self.get_answer(orchestrator, task))
        _ = self.get_function_args(orchestrator.performance_evaluation_log)
        _ = self.get_skills_responses(orchestrator.performance_evaluation_log)
        _ = self.get_skill_args(
            orchestrator.performance_evaluation_log["skill_required_arguments_schema"]
        )
        _ = self.get_skill_descriptions(
            orchestrator.performance_evaluation_log["skill_required_arguments_schema"]
        )
        return result.answer
```

### Feedback Definitions and Recording

```js
f_open_ai = fOpenAI(
    model_engine=os.getenv("OPEN_AI_CHAT_MODEL"),
    api_key=os.getenv("OPEN_AI_API_KEY"),
)
fn_eval = FunctionsEvaluation(provider=f_open_ai)
f_fn_args_eval = (
   Feedback(
       fn_eval.evaluate_function_arguments,
       name="Function Arguments Evaluation",
   )
   .on(Select.RecordCalls.get_function_args.rets)
   .on(Select.RecordCalls.get_skill_args.rets)
)  # instantiate feedback function
f_moderation = Feedback(fn_eval.evaluate_moderation, name="Moderation Evaluation").on_output()
grounded = Groundedness(groundedness_provider=f_open_ai)
# Define a groundedness feedback function
f_groundedness = (
    Feedback(grounded.groundedness_measure_with_cot_reasons, name="Groundedness")
    .on(Select.RecordCalls.get_skills_responses.rets)
    .on_output()
    .aggregate(grounded.grounded_statements_aggregator)
)
f_qa_relevance = (
    Feedback(f_open_ai.relevance_with_cot_reasons, name="Answer Relevance")
    .on_input()
    .on_output()
)
f_fn_choice_eval = (
    Feedback(
       fn_eval.evaluate_function_choice_with_cot_reasons,
       name="Function Choice Evaluation",
    )
    .on_input()
    .on(Select.RecordCalls.get_function_args.rets)
    .on(Select.RecordCalls.get_skill_descriptions.rets)
)
f_fn_guideline_adherence = (
    Feedback(
        fn_eval.evaluate_guidelines_adherence_with_cot_reasons,
        name="Response Guideline Adherence",
     )
     .on_input()
     .on_output()
     .on(Select.RecordCalls.get_skills_responses.rets)
)

with tru_custom_app as recording:
    orchestrator.query(question, chat)
record = recording.get()
feedback_results = tru.run_feedback_functions(
    record=record,
    feedback_functions=[
        f_fn_args_eval,
        f_groundedness,
        f_qa_relevance,
        f_fn_choice_eval,
        f_fn_guideline_adherence,
        f_moderation,
   ],
)
tru.add_feedbacks(feedback_results)

```

## Custom Metrics in Datasheet Assistant

<table class="table-size-for-cloud-services">
    <thead>
        <tr>
            <th>Metrics</th>
            <th>Description</th>
        </tr>
    </thead>
    <tbody>
        <tr>
            <td class="custom-header">Expected Answer Match or Conceptual Information Overlap</td>
            <td>This metric compares the overlap of information between a source and a statement. It returns a score between 0 and 1, indicating the degree of overlap.</td>
        </tr>
        <tr>
            <td class="custom-header">Conversation Starter Presence</td>
            <td>This metric evaluates the presence of conversation starters in a given text. It returns a score between 0 and 1, indicating the presence and absence of conversation starters respectively.</td>
        </tr>
        <tr>
            <td class="custom-header">Search Query Accuracy</td>
            <td>This metric evaluates the accuracy of a search query generated based on a user question and chat history. It returns a score between 0 and 1, indicating the accuracy of the search query.</td>
        </tr>
        <tr>
            <td class="custom-header">Search Query Conciseness</td>
            <td>This metric evaluates the conciseness of a search query generated based on a user question and chat history. It returns a score between 0 and 1, indicating the conciseness of the search query.</td>
        </tr>
    </tbody>
</table>

### Metrics Definition with Prompts

````js
COMPARE_ANSWER_TEMPLATE = """
System:
You are an INFORMATION / CONCEPTUAL OVERLAP classifier providing the overlap of information between a SOURCE and STATEMENT.
Use the 'Conceptual Overlap' model to compare the given actual answer with given expected answer to see how much does actual answer matches.
Respond only as a number between 0-10 where 0 is no information overlap and 10 is all information is overlapping

A few additional scoring guidelines:

- Long RESPONSES should score equally well as short RESPONSES.

- Score should increase as the RESPONSE contains more information from the EXPECTED RESPONSE.

- RESPONSE that does not contain any overlapping information from the EXPECTED RESPONSE should get a score of 0.

- RESPONSE that contains some overlapping information from the EXPECTED RESPONSE should get as score of 2, 3, or 4. Higher score indicates more overlapping informations.

- RESPONSE that contains more overlapping information from the EXPECTED RESPONSE should get a score between a 5, 6, 7 or 8. Higher score indicates more overlapping informations.

- RESPONSE that almost contains most of the overlapping information from the EXPECTED RESPONSE should get a score of 9 or 10.

- RESPONSE must contain all informations present in the EXPECTED RESPONSE to get a score of 10.

- RESPONSE might contain additional information that is not present in the EXPECTED RESPONSE. This should not affect the score.

- RESPONSE that confidently FALSE should get a score of 0.

- RESPONSE that is only seemingly MATCH should get a score of 0.


- Never elaborate.

EXPECTED RESPONSE (mentioned within the triple quotes below):
```{expected_answer}```

RESPONSE (mentioned within the triple quotes below):
```{actual_answer}```

Score: """


CONVERSATIONAL_WORDS_PRESENCE_TEMPLATE = """
System:
You are a CONVERSATION WORDS PRESENCE classifier providing the absence of CONVERSATION WORDS in the given TEXT.
You will be provided with a TEXT.
Your task is to evaluate the absence of CONVERSATION WORDS in the TEXT.
Conversation starters are informal phrases or questions that are used to initiate or continue a conversation. They are not directly related to the main topic of the text.

Please follow these guidelines for your evaluation:
1. Give a score of 0 if the SEARCH QUERY contains informal language, conversation starters.
2. Give a score of 10 if the SEARCH QUERY does not contain informal language, conversation starters.

TEXT (mentioned within the triple quotes below):
```{text}```

Please answer using the entire template below.

TEMPLATE:
Observation: <The observation on the absence of CONVERSATION WORDS in the TEXT.>
Score: <The score 0-10 based on the absence of CONVERSATION WORDS.>
Supporting Evidence: <Provide your reasons for scoring based on the absence of CONVERSATION WORDS. Tie it back to the evaluation being completed.>
"""

SEARCH_QUERY_ACCURACY_TEMPLATE = """

System:
You are a SEARCH QUERY ACCURACY classifier providing the accuracy of the SEARCH QUERY formed by combination of USER QUESTION and CHAT HISTORY.
You will be provided with a USER QUESTION, CHAT HISTORY and a SEARCH QUERY.
Your task is to evaluate If the SEARCH QUERY accurately reflects the USER QUESTION and includes all necessary details to be understood on its own. This includes appropriately filling in any gaps left by an incomplete USER QUESTION using the CHAT HISTORY.
Purpose of the SEARCH QUERY is to retrieve the desired information from the database related to the USER QUESTION.

USER QUESTION (mentioned within the triple quotes below):
```{user_question}```

CHAT HISTORY (mentioned within the triple quotes below):
```{chat_history}```

SEARCH QUERY (mentioned within the triple quotes below):
```{search_query}```

Please answer using the entire template below.

TEMPLATE:
Observation: <The observation on the criteria for the SEARCH QUERY ACCURACY evaluation.>
Score: <The score 0-10 based on the given criteria>
Supporting Evidence: <Provide your reasons for scoring based on the ACCURACY of the SEARCH QUERY. Tie it back to the evaluation being completed.>
"""

SEARCH_QUERY_CONCISENESS_TEMPLATE = """

System:
You are a SEARCH QUERY CONCISENESS classifier providing the conciseness of the SEARCH QUERY formed by combination of USER QUESTION and CHAT HISTORY.
You will be provided with a USER QUESTION, CHAT HISTORY and a SEARCH QUERY.
Your task is to evaluate If the SEARCH QUERY is free from irrelevant or extraneous information, focusing solely on what is necessary to answer the USER QUESTION effectively.
Purpose of the SEARCH QUERY is to retrieve the desired information from the database related to the USER QUESTION.

USER QUESTION (mentioned within the triple quotes below):
```{user_question}```

CHAT HISTORY (mentioned within the triple quotes below):
```{chat_history}```

SEARCH QUERY (mentioned within the triple quotes below):
```{search_query}```

Please answer using the entire template below.

TEMPLATE:
Observation: <The observation on the criteria for the SEARCH QUERY CONCISENESS evaluation.>
Score: <The score 0-10 based on the given criteria>
Supporting Evidence: <Provide your reasons for scoring based on the CONCISENESS of the SEARCH QUERY. Tie it back to the evaluation being completed.>
"""


class ExtendedOpenAIProvider(OpenAI):

    def compare_using_conceptual_overlap_with_cot_reasons(
        self, expected_answer: str, actual_answer: str
    ) -> float:
        """Get the comparision feedback using conceptual overlap
        Uses chat completion Model. Also uses chain of
        thought methodology and emits the reasons.

        Usage:
            ``` python
            feedback = (
                Feedback(provider.compare_using_conceptual_overlap_with_cot_reasons)
                .on(expected_answer=Select.RecordCalls.query.args["expected_answer"])
                .on(actual_answer=Select.RecordCalls.query.rets)
            )

        Args:
            expected_answer (str): The expected answer for the current question in test
            actual_answer (str): The actual answer from the assistant

        Returns:
            float: A value between 0 and 1. 0 being "not same" and 1 being "mostly same".
        """

        system_prompt = str.format(
            COMPARE_ANSWER_TEMPLATE,
            expected_answer=expected_answer,
            actual_answer=actual_answer,
        )
        system_prompt = system_prompt.replace("Score:", prompts.COT_REASONS_TEMPLATE)
        return self.generate_score_and_reasons(system_prompt)

    def chat_conversation_presence_with_cot_reasons(self, text: str) -> float:
        """Get the presence of conversation starters in the text

        Usage:
            ```python
            conversation_starters_feedback = (
                Feedback(provider.chat_conversation_presence_with_cot_reasons, name="Conversation Starters")
                .on(text=Select.RecordCalls.get_search_query.rets)
            )

        Args:
            text (str): The text to be evaluated for conversation starters

        Returns:
            float: A value between 0 and 1. 0 being "no conversation starters" and 1 being "conversation starters present".
        """

        system_prompt = str.format(
            CONVERSATIONAL_WORDS_PRESENCE_TEMPLATE,
            text=text,
        )
        return self.generate_score_and_reasons(system_prompt)

    def search_query_accuracy_with_cot_reasons(
        self, user_question: str, chat_history: list[str], search_query: str
    ) -> float:
        """Get the accuracy score for the search query generated

        Usage:
            ```python
            search_query_accuracy_feedback = (
                Feedback(provider.search_query_accuracy_with_cot_reasons, name="Search Query Accuracy")
                .on(user_question=Select.RecordCalls.query.args["question"])
                .on(chat_history=Select.RecordCalls.query.args["chat_history"])
                .on(search_query=Select.RecordCalls.get_search_query.rets)
            )

        Args:
            user_question (str): The user question for which the query is being generated
            chat_history (list[str]): previous chat history
            search_query (str): The search query generated by the system

        Returns:
            float: A value between 0 and 1. 0 being "not accurate" and 1 being "mostly accurate".
        """

        system_prompt = str.format(
            SEARCH_QUERY_ACCURACY_TEMPLATE,
            user_question=user_question,
            chat_history=chat_history,
            search_query=search_query,
        )
        return self.generate_score_and_reasons(system_prompt)

    def search_query_conciseness_with_cot_reasons(
        self, user_question: str, chat_history: list[str], search_query: str
    ) -> float:
        """Get the conciseness score for the search query generated

        Usage:
            ```python
            search_query_conciseness_feedback = (
                Feedback(provider.search_query_conciseness_with_cot_reasons, name="Search Query Conciseness")
                .on(user_question=Select.RecordCalls.query.args["question"])
                .on(chat_history=Select.RecordCalls.query.args["chat_history"])
                .on(search_query=Select.RecordCalls.get_search_query.rets)
            )

        Args:
            user_question (str): The user question for which the query is being generated
            chat_history (list[str]): previous chat history
            search_query (str): The search query generated by the system

        Returns:
            float: A value between 0 and 1. 0 being "not concise" and 1 being "mostly concise".
        """

        system_prompt = str.format(
            SEARCH_QUERY_CONCISENESS_TEMPLATE,
            user_question=user_question,
            chat_history=chat_history,
            search_query=search_query,
        )
        return self.generate_score_and_reasons(system_prompt)
````
