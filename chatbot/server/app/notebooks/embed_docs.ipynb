{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Github_personal\\Knowledge_base_FB\\chatbot\\server\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.05168594419956207,\n",
       " -0.030764883384108543,\n",
       " -0.03062233328819275,\n",
       " -0.02802734449505806,\n",
       " 0.01813092641532421]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv, find_dotenv\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "import os\n",
    "\n",
    "load_dotenv(find_dotenv())\n",
    "\n",
    "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\",google_api_key=os.getenv(\"GEMINI_API_KEY\"))\n",
    "vector = embeddings.embed_query(\"hello, world!\")\n",
    "vector[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'filepath': 'Azure360\\\\Azure Best Practices\\\\Management Infrastructure.md'}, page_content='| Resource quotas and limits | Enforce [resource quotas and limits](https://learn.microsoft.com/en-us/azure/azure-resource-manager/management/azure-subscription-service-limits) to control usage and costs within specific projects, ensuring resources are allocated appropriately. |')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter, Language\n",
    "from langchain_core.documents import Document\n",
    "import os\n",
    "\n",
    "knowledge_base_path = r\"D:\\Github_personal\\Knowledge_base_FB\\chatbot\\client\\docs\" #os.path.join(\"../../../../knowledge_base\")\n",
    "\n",
    "seperators = RecursiveCharacterTextSplitter.get_separators_for_language(Language.MARKDOWN)\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=300,\n",
    "    chunk_overlap=20,\n",
    "    separators=seperators\n",
    ")\n",
    "\n",
    "docs = []\n",
    "\n",
    "filepaths = []\n",
    "for root, dirs, files in os.walk(knowledge_base_path):\n",
    "    for file in files:\n",
    "        if not file.endswith(\".md\"):\n",
    "            continue\n",
    "        # get path relative to knowledge_base\n",
    "        filepath = os.path.join(root, file)\n",
    "        filepath = os.path.relpath(filepath, knowledge_base_path)\n",
    "        filepaths.append(filepath)\n",
    "\n",
    "def get_docs_from_file_paths(filepaths):\n",
    "    docs = []\n",
    "    for filepath in filepaths:\n",
    "        with open(os.path.join(knowledge_base_path, filepath), \"r\",encoding=\"utf-8\") as f:\n",
    "            content = f.read()\n",
    "        chunks = splitter.split_text(content)\n",
    "        docs.extend([Document(page_content=chunk, metadata={\"filepath\":filepath}) for chunk in chunks])\n",
    "    return docs\n",
    "\n",
    "docs = get_docs_from_file_paths(filepaths)\n",
    "docs[100]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5467"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "embeddings_path = r\"D:\\Github_personal\\Knowledge_base_FB\\chatbot\\embeddings\"#os.path.join(knowledge_base_path,\"..\",\"embeddings\")\n",
    "\n",
    "index = FAISS.from_documents(docs, embeddings)\n",
    "index.save_local(embeddings_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = FAISS.load_local(\n",
    "    embeddings_path, embeddings, allow_dangerous_deserialization=True\n",
    ")\n",
    "retriever = index.as_retriever(\n",
    "    search_type=\"similarity_score_threshold\",\n",
    "    search_kwargs={\"k\": 5, \"score_threshold\": 0.5},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'filepath': 'AI360\\\\Overview.md'}, page_content='## Purpose of RAG360'),\n",
       " Document(metadata={'filepath': 'DevOps\\\\Kubernites\\\\Overview.md'}, page_content='## Purpose of RAG360'),\n",
       " Document(metadata={'filepath': 'RAG360\\\\Overview.md'}, page_content='## Purpose of RAG360'),\n",
       " Document(metadata={'filepath': 'RAG360\\\\Introduction.md'}, page_content='## Purpose of RAG360\\n1. Knowledge on AI is not tied to an engineer or project, but available in the company for everyone to access and improve on.\\n2. Collection of all knowledge that is gained by the Venture-AI team through the exploration and projects.'),\n",
       " Document(metadata={'filepath': 'RAG360\\\\Introduction.md'}, page_content='import ZoomImage from \"../../src/components/Zooming/ZoomImage\";\\n\\n**RAG360** is a comprehensive resource designed,')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = retriever.invoke(\"Rag360\")\n",
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Steps\n",
    "# 1. get last embedding commit hash from /embeddings/commit_hash.txt\n",
    "# 2. get the file diff from the last commit hash to the current commit hash\n",
    "# 3. get the files that were added or modified or deleted\n",
    "# 4. delete the embeddings of the added/modified/deleted files from the index\n",
    "# 5. get the embeddings of the added and modified files from current commit hash\n",
    "# 6. add the embeddings of the added and modified files to the index\n",
    "# 7. save the current commit hash to /embeddings/commit_hash.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'AI\\\\ThinkPad.md', 'ThinkPad.md', 'Devops.md', 'Python\\\\Books.md', 'Python\\\\Profiling.md', 'General.md', 'Python\\\\Python.md'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'cc245121dec183aad09d143646c79c5740e6c652'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from git import Repo\n",
    "import os\n",
    "\n",
    "# Path to your repository\n",
    "repo_path = '../../../../'\n",
    "repo = Repo(repo_path)\n",
    "\n",
    "# Path to the commit hash file\n",
    "commit_hash_file = os.path.join(embeddings_path, 'commit_hash.txt')\n",
    "\n",
    "# Read the last commit hash\n",
    "with open(commit_hash_file, 'r') as file:\n",
    "    last_commit_hash = file.read().strip()\n",
    "\n",
    "# Get the diff from the last commit hash to the current commit hash\n",
    "diff_index = repo.commit(last_commit_hash).diff('HEAD')\n",
    "\n",
    "# Get the diff file paths\n",
    "affected_markdown_files = set(os.path.normpath(item.a_path.replace('knowledge_base/', ''))\n",
    "                               for item in diff_index \n",
    "                               if item.a_path.startswith('knowledge_base') and item.a_path.endswith('.md'))\n",
    "affected_markdown_files\n",
    "\n",
    "def get_doc_ids_from_filepaths(filepaths):\n",
    "    doc_ids = []\n",
    "    for doc_id in index.index_to_docstore_id.values():\n",
    "        doc = index.docstore.search(doc_id)\n",
    "        if doc.metadata['filepath'] in filepaths:\n",
    "            doc_ids.append(doc_id)\n",
    "    return doc_ids\n",
    "\n",
    "# Get the doc ids of the affected markdown files\n",
    "doc_ids_to_delete = get_doc_ids_from_filepaths(affected_markdown_files)\n",
    "\n",
    "def get_current_affected_files():\n",
    "    current_affected_files = set()\n",
    "    for root, dirs, files in os.walk(knowledge_base_path):\n",
    "        for file in files:\n",
    "            if not file.endswith(\".md\"):\n",
    "                continue\n",
    "            filepath = os.path.join(root, file)\n",
    "            # get path relative to knowledge_base\n",
    "            filepath = os.path.relpath(filepath, knowledge_base_path)\n",
    "            if filepath in affected_markdown_files:\n",
    "                current_affected_files.add(filepath)\n",
    "    return current_affected_files\n",
    "\n",
    "doc_ids_to_delete\n",
    "\n",
    "print(affected_markdown_files)\n",
    "current_affected_files = get_current_affected_files()\n",
    "docs_to_add = get_docs_from_file_paths(current_affected_files)\n",
    "len(docs_to_add)\n",
    "\n",
    "latest_commit_hash = repo.head.object.hexsha\n",
    "latest_commit_hash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['README.md', 'knowledge_base/AI/ThinkPad.md', 'knowledge_base/General.md', 'chatbot/README.md', 'knowledge_base/Python/Books.md', 'knowledge_base/ThinkPad.md', 'scripts/README_with_placeholders.md', 'CONTRIBUTING.md', 'CONTRIBUTORS.md', 'knowledge_base/Python/Python.md', 'knowledge_base/Devops.md', 'knowledge_base/Python/Profiling.md']\n"
     ]
    }
   ],
   "source": [
    "affected_markdown_files\n",
    "normalized_filepaths = [filepath for filepath in affected_markdown_files]\n",
    "print(normalized_filepaths)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
